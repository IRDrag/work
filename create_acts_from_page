import os
import requests
import json
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time

ACTS_PAGE_URL = "Тут нужная страничка"
BASE_URL = "https://irkobl.ru"
OUTPUT_FILE_PATH = os.path.join("output_data", "datasets", "New.json")
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def clean_text(text):
    return ' '.join(text.strip().split())

def parse_legal_acts_v2():
    print(f"Захожу на страницу: {ACTS_PAGE_URL}")
    try:
        response = requests.get(ACTS_PAGE_URL, headers=HEADERS, timeout=20)
        response.raise_for_status()
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
    except requests.exceptions.RequestException as e:
        print(f"Ошибка: Не удалось получить доступ к странице. {e}")
        return

    legal_acts_list = []
    content_area = soup.find('div', {'class': 'news-list'})
    if not content_area:
        content_area = soup # Если не нашли, ищем по всей странице

    for block in content_area.find_all(['p', 'div']):
        link = block.find('a', href=True)
        
        if link and any(link['href'].lower().endswith(ext) for ext in ['.pdf', '.doc', '.docx', '.rtf']):
            title = clean_text(block.get_text())
            full_url = urljoin(BASE_URL, link['href'])
            
            if not any(act['url'] == full_url for act in legal_acts_list):
                act = {"title": title, "url": full_url}
                legal_acts_list.append(act)
                print(f"  [+] Добавлен документ: {title[:70]}...")

    if not legal_acts_list:
        print("Не удалось найти ни одного документа на странице.")
        return
        
    with open(OUTPUT_FILE_PATH, 'w', encoding='utf-8') as f:
        json.dump(legal_acts_list, f, ensure_ascii=False, indent=4)
        
    print(f"\nГотово! Создан файл {OUTPUT_FILE_PATH} с {len(legal_acts_list)} документами.")

if __name__ == "__main__":
    os.makedirs(os.path.join("output_data", "datasets"), exist_ok=True)
    parse_legal_acts_v2()
